{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zyJ25uz0kSaw"
   },
   "source": [
    "# Natural Language Processing\n",
    "\n",
    "### Ruthu S Sanketh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ao1nhg9RknmF"
   },
   "source": [
    "The central idea of this tutorial is to explore LSTM based models. We will explore how the size of the model effects the sequence generated. We will see both character based and word based models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IXdkhxZAXnTW"
   },
   "source": [
    "# Word Based LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "FbU5DRolXseI"
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import keras\n",
    "import string\n",
    "import nltk\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Embedding\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import np_utils\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "string.punctuation = string.punctuation + '“' + '”' +'-' + '’' + '‘' + '—'\n",
    "string.punctuation = string.punctuation.replace('.', '')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d2NR3RFFYOT8"
   },
   "source": [
    "We do basic pre processing which includes lowering etc. after looking at the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iQvfF2NjXxGj",
    "outputId": "e09902e8-6813-44fe-8009-650ee6aa132d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of sentences is 981\n",
      "The number of tokens is 26381\n",
      "The average number of tokens per sentence is 27\n",
      "The number of unique tokens are 3037\n"
     ]
    }
   ],
   "source": [
    "# Loads the data and preprocesses data and stores corpus in raw_text\n",
    "raw_text = open('/content/sample_data/corpus.txt', encoding = 'utf8').read()\n",
    "\n",
    "file_nl_removed = \"\"\n",
    "for line in raw_text:\n",
    "  line_nl_removed = line.replace(\"\\n\", \" \")           #removes newlines\n",
    "  file_nl_removed += line_nl_removed\n",
    "\n",
    "file_p = \"\".join([char for char in file_nl_removed if char not in string.punctuation])   #removes all special characters\n",
    "sents = nltk.sent_tokenize(file_p)\n",
    "print(\"The number of sentences is\", len(sents)) #prints the number of sentences\n",
    "\n",
    "string.punctuation = string.punctuation + '.'\n",
    "file_q = \"\".join([char for char in file_p if char not in string.punctuation])   #removes even periods.\n",
    "words = nltk.word_tokenize(file_q)\n",
    "print(\"The number of tokens is\", len(words)) #prints the number of tokens\n",
    "\n",
    "average_tokens = round(len(words)/len(sents))\n",
    "print(\"The average number of tokens per sentence is\", average_tokens) #prints the average number of tokens per sentence\n",
    "\n",
    "unique_tokens = set(words)\n",
    "print(\"The number of unique tokens are\", len(unique_tokens)) #prints the number of unique tokens\n",
    "\n",
    "preprocessed_text = file_p.lower()       #converts corpus into lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "eug68GOecM8Z"
   },
   "outputs": [],
   "source": [
    "# Hyperparameters of the model\n",
    "vocab_size = 2750  #chosen based on statistics of the model\n",
    "oov_tok = '<OOV>'\n",
    "embedding_dim = 100\n",
    "padding_type='post'\n",
    "trunc_type='post'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "bWNBOlJ5cQym"
   },
   "outputs": [],
   "source": [
    "# tokenizes sentences\n",
    "tokenizer = Tokenizer(num_words = vocab_size, oov_token=oov_tok)\n",
    "tokenizer.fit_on_texts([preprocessed_text])\n",
    "word_index = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "LNRJDbFcdHbO"
   },
   "outputs": [],
   "source": [
    "seq_length = 50\n",
    "tokens = tokenizer.texts_to_sequences([preprocessed_text])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ykfI4FrwdyJe",
    "outputId": "cd72a429-f66f-488b-a563-ba1088e8638a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training data size is - 26333\n"
     ]
    }
   ],
   "source": [
    "dataX = []\n",
    "dataY = []\n",
    "\n",
    "for i in range(0, len(tokens) - seq_length-1 , 1):\n",
    "  seq_in = tokens[i:i + seq_length]\n",
    "  seq_out = tokens[i + seq_length]\n",
    "\n",
    "  if seq_out==1: #Skip samples where target word is OOV\n",
    "    continue\n",
    "    \n",
    "  dataX.append(seq_in)\n",
    "  dataY.append(seq_out)\n",
    " \n",
    "N = len(dataX)\n",
    "print (\"Total training data size is -\", N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "cJmGr1xId8cO"
   },
   "outputs": [],
   "source": [
    "X = numpy.array(dataX)\n",
    "\n",
    "# one hot encodes the output variable\n",
    "y = numpy.array(dataY)\n",
    "y = np_utils.to_categorical(dataY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8QPApRA-d9JV",
    "outputId": "ab62c49f-bc96-447b-e818-0c2e470f152e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 50, 100)           275000    \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, 128)               84480     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 2750)              354750    \n",
      "=================================================================\n",
      "Total params: 714,230\n",
      "Trainable params: 714,230\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# with embedding\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Embedding(vocab_size, embedding_dim, input_length=seq_length),\n",
    "    keras.layers.Bidirectional(keras.layers.LSTM(64)),\n",
    "    keras.layers.Dense(vocab_size, activation='softmax')\n",
    "])\n",
    "\n",
    "# compiles model\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "14ClAAYpeCVO",
    "outputId": "c1268da8-92a3-41e1-ea95-73088bbf510a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "165/165 [==============================] - 3s 19ms/step - loss: 6.4300 - accuracy: 0.0549 - val_loss: 6.2585 - val_accuracy: 0.0849\n",
      "Epoch 2/5\n",
      "165/165 [==============================] - 3s 15ms/step - loss: 5.9578 - accuracy: 0.0560 - val_loss: 6.1912 - val_accuracy: 0.0849\n",
      "Epoch 3/5\n",
      "165/165 [==============================] - 2s 15ms/step - loss: 5.8401 - accuracy: 0.0582 - val_loss: 6.1955 - val_accuracy: 0.0845\n",
      "Epoch 4/5\n",
      "165/165 [==============================] - 3s 15ms/step - loss: 5.7421 - accuracy: 0.0639 - val_loss: 6.1913 - val_accuracy: 0.0932\n",
      "Epoch 5/5\n",
      "165/165 [==============================] - 2s 15ms/step - loss: 5.6465 - accuracy: 0.0687 - val_loss: 6.1742 - val_accuracy: 0.0951\n"
     ]
    }
   ],
   "source": [
    "# Uses validation split of 0.2 while training\n",
    "num_epochs = 5\n",
    "history = model.fit(X, y, epochs=num_epochs, batch_size = 128, verbose=1, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "mg9WSEwYeMAH"
   },
   "outputs": [],
   "source": [
    "#Creates word to idx map using tokenizer.word_index\n",
    "reverse_word_map = dict(map(reversed, tokenizer.word_index.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "A_mhL0J0eQku"
   },
   "outputs": [],
   "source": [
    "# Returns the next n words greedily\n",
    "def next_tokens(input_str, n):\n",
    "    print (\"Seed -\",  input_str, sep = '\\n\\n')\n",
    "    final_string = ''\n",
    "    for i in range(n):\n",
    "        token = tokenizer.texts_to_sequences([input_str])[0]\n",
    "        prediction = model.predict(token, verbose=0)\n",
    "        final_string = final_string + reverse_word_map[numpy.argmax(prediction[0])] + ' ' \n",
    "        input_str = input_str + ' ' + reverse_word_map[numpy.argmax(prediction[0])]\n",
    "        input_str = ' '.join(input_str.split(' ')[1:])\n",
    "    return final_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gZ_NTQezeWYO",
    "outputId": "8f10fe5f-827b-4283-d79e-e6f376225f5b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed -\n",
      "\n",
      "must remember remarked the king or ill have you executed the miserable hatter dropped his teacup and breadandbutter and went down on one knee im a poor man your majesty he began youre a very poor speaker said the king here one of the guineapigs cheered and was immediately suppressed\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 50) for input Tensor(\"embedding_input:0\", shape=(None, 50), dtype=float32), but it was called on an input with incompatible shape (None, 1).\n",
      "\n",
      "Generated string -\n",
      "\n",
      " the the little little be little little little little the \n"
     ]
    }
   ],
   "source": [
    "# picks a random seed\n",
    "start = numpy.random.randint(0, len(dataX)-1)\n",
    "pattern = dataX[start]\n",
    "input_str = ' '.join([reverse_word_map[value] for value in pattern])\n",
    "\n",
    "output = next_tokens(input_str, 10)\n",
    "print(\"\\nGenerated string -\\n\\n\", output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "u-x8W-OSVgNi",
    "outputId": "b9332b15-fb7b-4b84-9755-f5876ae22442"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed -\n",
      "\n",
      "The boy laughed at the fright he had caused. This time, the villagers left angrily. The third day, as the boy went up the small hill, he suddenly saw a wolf attacking his sheep. He cried as hard as he could, “Wolf! Wolf! Wolf!”, but not  a single villager came to help him. The villagers thought that he was trying to fool them again and did not come to rescue  him or his sheep.\n",
      "\n",
      "Generated string -\n",
      "\n",
      " little the the the little little little little the little \n"
     ]
    }
   ],
   "source": [
    "input_str = \"The boy laughed at the fright he had caused. This time, the villagers left angrily. The third day, as the boy went up\\\n",
    " the small hill, he suddenly saw a wolf attacking his sheep. He cried as hard as he could, “Wolf! Wolf! Wolf!”, but not \\\n",
    " a single villager came to help him. The villagers thought that he was trying to fool them again and did not come to rescue \\\n",
    " him or his sheep.\"\n",
    "\n",
    "# Uses first 50 tokens from given input_str as input. Since the seq_length is 50, only 50 tokens are taken using the tokenizer.\n",
    "output = next_tokens(input_str, 10)\n",
    "print(\"\\nGenerated string -\\n\\n\", output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B5_R5Tngo3_D"
   },
   "source": [
    "# Character based LSTM Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "2zZaHsejo57p"
   },
   "outputs": [],
   "source": [
    "# Uses the preprocessed data and create raw_text\n",
    "raw_text = preprocessed_text   #periods have not been removed for better results\n",
    "\n",
    "# creates mapping of unique characters to integers\n",
    "chars = sorted(list(set(raw_text)))\n",
    "char_to_int = dict((c, i) for i, c in enumerate(chars))\n",
    "int_to_char = dict((i, c) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lkVVDbump0Wg",
    "outputId": "b3249230-f7eb-4cef-d4c0-e79a6a6524a8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of total characters are 135030\n",
      "\n",
      "The character vocab size is 29\n"
     ]
    }
   ],
   "source": [
    "# Prints the total characters and character vocab size\n",
    "n_chars = len(raw_text)\n",
    "n_vocab = len(chars)\n",
    "\n",
    "print(\"The number of total characters are\", n_chars)\n",
    "print(\"\\nThe character vocab size is\", n_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2aVserymqE1b",
    "outputId": "9f1e468c-1f8e-4aae-da1a-96c52f95b1c8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Patterns:  134930\n"
     ]
    }
   ],
   "source": [
    "#Prepares dataset where the input is sequence of 100 characters and target is next character.\n",
    "seq_length = 100\n",
    "\n",
    "dataX = []\n",
    "dataY = []\n",
    "\n",
    "for i in range(0, n_chars - seq_length, 1):\n",
    "  seq_in = raw_text[i:i + seq_length]\n",
    "  seq_out = raw_text[i + seq_length]\n",
    "\n",
    "  dataX.append([char_to_int[char] for char in seq_in])\n",
    "  dataY.append(char_to_int[seq_out])\n",
    "\n",
    "n_patterns = len(dataX)\n",
    "print (\"Total Patterns: \", n_patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "Ic4yf4hNqc7T"
   },
   "outputs": [],
   "source": [
    "# reshapes X to be [samples, time steps, features]\n",
    "X = numpy.reshape(dataX, (n_patterns, seq_length, 1))\n",
    "\n",
    "# one hot encodes the output variable\n",
    "y = np_utils.to_categorical(dataY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "XvawnjFVqhMi"
   },
   "outputs": [],
   "source": [
    "embedding_dim =100\n",
    "max_length =100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ek5DqNeTqkAZ",
    "outputId": "dd0beb83-384f-43de-a626-b70c4b4cde6d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 100, 100)          2900      \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 256)               365568    \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 29)                7453      \n",
      "=================================================================\n",
      "Total params: 375,921\n",
      "Trainable params: 375,921\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(n_vocab, embedding_dim, input_length=max_length))\n",
    "model.add(LSTM(256))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gFoStJIOqpM6",
    "outputId": "50f1f19d-da96-4d7d-a85b-5fe64d160191"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1055/1055 [==============================] - 18s 17ms/step - loss: 2.0844\n",
      "Epoch 2/20\n",
      "1055/1055 [==============================] - 18s 17ms/step - loss: 1.6293\n",
      "Epoch 3/20\n",
      "1055/1055 [==============================] - 18s 17ms/step - loss: 1.4544\n",
      "Epoch 4/20\n",
      "1055/1055 [==============================] - 17s 17ms/step - loss: 1.3504\n",
      "Epoch 5/20\n",
      "1055/1055 [==============================] - 17s 17ms/step - loss: 1.2790\n",
      "Epoch 6/20\n",
      "1055/1055 [==============================] - 17s 17ms/step - loss: 1.2227\n",
      "Epoch 7/20\n",
      "1055/1055 [==============================] - 17s 16ms/step - loss: 1.1762\n",
      "Epoch 8/20\n",
      "1055/1055 [==============================] - 17s 17ms/step - loss: 1.1376\n",
      "Epoch 9/20\n",
      "1055/1055 [==============================] - 17s 16ms/step - loss: 1.0989\n",
      "Epoch 10/20\n",
      "1055/1055 [==============================] - 17s 16ms/step - loss: 1.0654\n",
      "Epoch 11/20\n",
      "1055/1055 [==============================] - 17s 16ms/step - loss: 1.0371\n",
      "Epoch 12/20\n",
      "1055/1055 [==============================] - 17s 16ms/step - loss: 1.0068\n",
      "Epoch 13/20\n",
      "1055/1055 [==============================] - 17s 17ms/step - loss: 0.9790\n",
      "Epoch 14/20\n",
      "1055/1055 [==============================] - 17s 17ms/step - loss: 0.9561\n",
      "Epoch 15/20\n",
      "1055/1055 [==============================] - 17s 16ms/step - loss: 0.9333\n",
      "Epoch 16/20\n",
      "1055/1055 [==============================] - 17s 16ms/step - loss: 0.9134\n",
      "Epoch 17/20\n",
      "1055/1055 [==============================] - 17s 16ms/step - loss: 0.8917\n",
      "Epoch 18/20\n",
      "1055/1055 [==============================] - 17s 16ms/step - loss: 0.8738\n",
      "Epoch 19/20\n",
      "1055/1055 [==============================] - 17s 16ms/step - loss: 0.8590\n",
      "Epoch 20/20\n",
      "1055/1055 [==============================] - 17s 16ms/step - loss: 0.8442\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f3aaab017f0>"
      ]
     },
     "execution_count": 25,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X, y, epochs = 20, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "OixPrw6vq15j"
   },
   "outputs": [],
   "source": [
    "#Gets the generated string using the model.\n",
    "def predict_next_n_chars(pattern, n):\n",
    "    for i in range(n):\n",
    "      x = numpy.reshape(pattern, (1, len(pattern), 1))\n",
    "      prediction = model.predict(x, verbose=0)\n",
    "      print (int_to_char[numpy.argmax(prediction)], end = '')   #get next char index.\n",
    "      seq_in = [int_to_char[value] for value in pattern]\n",
    "      pattern.append(numpy.argmax(prediction))\n",
    "      pattern = pattern[1:len(pattern)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vHH_I5QiUxnY",
    "outputId": "b81c0e0a-91b0-4957-f923-3910b7e00a69"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed -\n",
      "\n",
      "er sister as well as she could remember them all these strange adventures of hers that you have just\n",
      "\n",
      "Generated string -\n",
      "\n",
      " been that said the caterpillar. well i cant see the rest of the court and the mock turtle said and the mock turtle said and the mock turtle said and the mock turtle said and the mock turtle said and "
     ]
    }
   ],
   "source": [
    "#picks a random seed\n",
    "start = numpy.random.randint(0, len(dataX)-1)\n",
    "pattern = dataX[start]\n",
    "input_str = ''.join([int_to_char[value] for value in pattern])\n",
    "print (\"Seed -\",  input_str, sep = '\\n\\n')\n",
    "print (\"\\nGenerated string -\\n\")\n",
    "\n",
    "predict_next_n_chars(pattern, 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iutpuJAgrgU8",
    "outputId": "82bc7499-1e71-434b-d8aa-5a2a20a5d601"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed -\n",
      "\n",
      "the boy laughed at the fright he had caused. this time, the villagers left angrily. the third day, as the boy went up the small hill, he suddenly saw a wolf attacking his sheep. he cried as hard as he could, “wolf! wolf! wolf!”, but not  a single villager came to help him. the villagers thought that he was trying to fool them again and did not come to rescue  him or his sheep.\n",
      "\n",
      "Generated string -\n",
      "\n",
      "if she was a little before she had not the queen said to the mock turtle said and alice was so much alice thought to herself i wonder what a curious this time and she went on all the rest of the court"
     ]
    }
   ],
   "source": [
    "input_str = \"The boy laughed at the fright he had caused. This time, the villagers left angrily. The third day, as the boy went up\\\n",
    " the small hill, he suddenly saw a wolf attacking his sheep. He cried as hard as he could, “Wolf! Wolf! Wolf!”, but not \\\n",
    " a single villager came to help him. The villagers thought that he was trying to fool them again and did not come to rescue \\\n",
    " him or his sheep.\"\n",
    "\n",
    "#Uses the first 100 characters from given input_str as input to generate next 200 characters. \n",
    "input_str = input_str.lower()\n",
    "input_string = ''\n",
    "for each in input_str:\n",
    "  if each in chars:\n",
    "    if (len (input_string)<100):\n",
    "      input_string += each\n",
    "\n",
    "pattern = []\n",
    "pattern.append([char_to_int[char] for char in input_string])\n",
    "\n",
    "print (\"Seed -\",  input_str, sep = '\\n\\n')\n",
    "print (\"\\nGenerated string -\\n\")\n",
    "predict_next_n_chars(pattern[0], 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n8shbcukr0tJ"
   },
   "source": [
    "## Character based LSTM Model 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BWBPCrTdr46U",
    "outputId": "8ef3d368-a3fe-4653-ca87-dd6067dd7ef8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 100, 100)          2900      \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 100, 256)          365568    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100, 256)          0         \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 256)               525312    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 29)                7453      \n",
      "=================================================================\n",
      "Total params: 901,233\n",
      "Trainable params: 901,233\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model1 = Sequential()\n",
    "model1.add(Embedding(n_vocab, embedding_dim, input_length=max_length))\n",
    "model1.add(LSTM(256, input_shape=(X.shape[1], embedding_dim),return_sequences=True))\n",
    "model1.add(Dropout(0.2))\n",
    "model1.add(LSTM(256))\n",
    "model1.add(Dropout(0.2))\n",
    "model1.add(Dense(y.shape[1], activation='softmax'))\n",
    "model1.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "model1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7ZxrtjFIr63L",
    "outputId": "8b1bfb43-fe38-4dd8-baa9-932afa68ebd8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "2109/2109 [==============================] - 50s 24ms/step - loss: 1.9928\n",
      "Epoch 2/20\n",
      "2109/2109 [==============================] - 50s 24ms/step - loss: 1.5066\n",
      "Epoch 3/20\n",
      "2109/2109 [==============================] - 50s 24ms/step - loss: 1.3534\n",
      "Epoch 4/20\n",
      "2109/2109 [==============================] - 50s 24ms/step - loss: 1.2667\n",
      "Epoch 5/20\n",
      "2109/2109 [==============================] - 50s 24ms/step - loss: 1.2051\n",
      "Epoch 6/20\n",
      "2109/2109 [==============================] - 49s 23ms/step - loss: 1.1570\n",
      "Epoch 7/20\n",
      "2109/2109 [==============================] - 50s 23ms/step - loss: 1.1172\n",
      "Epoch 8/20\n",
      "2109/2109 [==============================] - 50s 24ms/step - loss: 1.0830\n",
      "Epoch 9/20\n",
      "2109/2109 [==============================] - 50s 24ms/step - loss: 1.0512\n",
      "Epoch 10/20\n",
      "2109/2109 [==============================] - 50s 23ms/step - loss: 1.0259\n",
      "Epoch 11/20\n",
      "2109/2109 [==============================] - 50s 24ms/step - loss: 1.0014\n",
      "Epoch 12/20\n",
      "2109/2109 [==============================] - 50s 24ms/step - loss: 0.9800\n",
      "Epoch 13/20\n",
      "2109/2109 [==============================] - 50s 24ms/step - loss: 0.9616\n",
      "Epoch 14/20\n",
      "2109/2109 [==============================] - 50s 24ms/step - loss: 0.9464\n",
      "Epoch 15/20\n",
      "2109/2109 [==============================] - 49s 23ms/step - loss: 0.9301\n",
      "Epoch 16/20\n",
      "2109/2109 [==============================] - 49s 23ms/step - loss: 0.9175\n",
      "Epoch 17/20\n",
      "2109/2109 [==============================] - 50s 24ms/step - loss: 0.9036\n",
      "Epoch 18/20\n",
      "2109/2109 [==============================] - 49s 23ms/step - loss: 0.8934\n",
      "Epoch 19/20\n",
      "2109/2109 [==============================] - 49s 23ms/step - loss: 0.8855\n",
      "Epoch 20/20\n",
      "2109/2109 [==============================] - 49s 23ms/step - loss: 0.8742\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f3a154a4e80>"
      ]
     },
     "execution_count": 30,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.fit(X, y, epochs = 20, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "L6RTfHN6r8wh"
   },
   "outputs": [],
   "source": [
    "# Generates the sequence similar to above methods. Gets the generated string using the model.\n",
    "def predict_next_n_chars(pattern, n):\n",
    "    for i in range(n):\n",
    "      x = numpy.reshape(pattern, (1, len(pattern), 1))\n",
    "      prediction = model1.predict(x, verbose=0)\n",
    "      print (int_to_char[numpy.argmax(prediction)], end = '')   #get next char index.\n",
    "      seq_in = [int_to_char[value] for value in pattern]\n",
    "      pattern.append(numpy.argmax(prediction))\n",
    "      pattern = pattern[1:len(pattern)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5CKJXZ4tYmL0",
    "outputId": "eeac175e-be71-481d-d1b3-1f107c916e21"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed -\n",
      "\n",
      "it in time said the caterpillar and it put the hookah into its mouth and began smoking again. this t\n",
      "\n",
      "Generated string -\n",
      "\n",
      "ime the mouse was a large cat said the mouse to herself it was a little thing and the moral of that is that the mouse was a little birds with a tree in a low voice of the edge of the mouse who was not"
     ]
    }
   ],
   "source": [
    "#picks a random seed\n",
    "start = numpy.random.randint(0, len(dataX)-1)\n",
    "pattern = dataX[start]\n",
    "input_str = ''.join([int_to_char[value] for value in pattern])\n",
    "print (\"Seed -\",  input_str, sep = '\\n\\n')\n",
    "print (\"\\nGenerated string -\\n\")\n",
    "\n",
    "predict_next_n_chars(pattern, 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YyRCfLe5YmL7",
    "outputId": "414d05bd-e7b1-4e76-e775-36c34e9801a6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed -\n",
      "\n",
      "the boy laughed at the fright he had caused. this time, the villagers left angrily. the third day, as the boy went up the small hill, he suddenly saw a wolf attacking his sheep. he cried as hard as he could, “wolf! wolf! wolf!”, but not  a single villager came to help him. the villagers thought that he was trying to fool them again and did not come to rescue  him or his sheep.\n",
      "\n",
      "Generated string -\n",
      "\n",
      "she spoke and the moral of that is that the mouse was a little birds with a tree in a low voice of the edge of the mouse who was not a mouse that was the mouse doesnt matter the white rabbit while she"
     ]
    }
   ],
   "source": [
    "input_str = \"The boy laughed at the fright he had caused. This time, the villagers left angrily. The third day, as the boy went up\\\n",
    " the small hill, he suddenly saw a wolf attacking his sheep. He cried as hard as he could, “Wolf! Wolf! Wolf!”, but not \\\n",
    " a single villager came to help him. The villagers thought that he was trying to fool them again and did not come to rescue \\\n",
    " him or his sheep.\"\n",
    "\n",
    "#Uses the first 100 characters from given input_str as input to generate next 200 characters. \n",
    "input_str = input_str.lower()\n",
    "input_string = ''\n",
    "for each in input_str:\n",
    "  if each in chars:\n",
    "    if (len (input_string)<100):\n",
    "      input_string += each\n",
    "\n",
    "pattern = []\n",
    "pattern.append([char_to_int[char] for char in input_string])\n",
    "\n",
    "print (\"Seed -\",  input_str, sep = '\\n\\n')\n",
    "print (\"\\nGenerated string -\\n\")\n",
    "predict_next_n_chars(pattern[0], 200)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Assignment 5 - 18AE30018.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
